{
  "1": {
    "inputs": {
      "seed": 392083131863893,
      "steps": 15,
      "cfg": 8,
      "sampler_name": "euler",
      "scheduler": "normal",
      "denoise": 0.96,
      "model": [
        "2",
        0
      ],
      "positive": [
        "12",
        0
      ],
      "negative": [
        "12",
        1
      ],
      "latent_image": [
        "3",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "2": {
    "inputs": {
      "ckpt_name": "SD_XL/juggernautXL_v9Rundiffusionphoto2.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "3": {
    "inputs": {
      "width": 1024,
      "height": 1024,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "4": {
    "inputs": {
      "text": "ng_deepnegative_v1_75t, (badhandv4:1.2),(holding:1.3),(worst quality:1.3),(low quality:1.2),(normal quality:1.2),lowres, bad anatomy, bad hands,flare,glow,hair covered face,(blur:1.25), (defocus:1.25),(depth of field:1.2),black and white photo,(bad eyes,:1.2),weird hue colors,CGI, Unreal, Airbrushed, Digital,bad teeth, metal, reflective, shiny, CGI, glow",
      "clip": [
        "2",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "12": {
    "inputs": {
      "strength": 0.85,
      "start_percent": 0,
      "end_percent": 1,
      "positive": [
        "17",
        0
      ],
      "negative": [
        "17",
        1
      ],
      "control_net": [
        "18",
        0
      ],
      "image": [
        "19",
        0
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "Apply ControlNet (Advanced)"
    }
  },
  "14": {
    "inputs": {
      "control_net_name": "diffusers_xl_depth_full.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "16": {
    "inputs": {
      "image": "/u/gdso/Pictures/syndata_test/imgs/images_0.png",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image RGB"
    }
  },
  "17": {
    "inputs": {
      "strength": 0.7000000000000001,
      "start_percent": 0,
      "end_percent": 1,
      "positive": [
        "162",
        0
      ],
      "negative": [
        "4",
        0
      ],
      "control_net": [
        "14",
        0
      ],
      "image": [
        "63",
        0
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "Apply ControlNet (Advanced)"
    }
  },
  "18": {
    "inputs": {
      "control_net_name": "diffusers_xl_canny_full.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "19": {
    "inputs": {
      "low_threshold": 100,
      "high_threshold": 200,
      "resolution": 1024,
      "image": [
        "22",
        0
      ]
    },
    "class_type": "CannyEdgePreprocessor",
    "_meta": {
      "title": "Canny Edge"
    }
  },
  "20": {
    "inputs": {
      "samples": [
        "1",
        0
      ],
      "vae": [
        "2",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "21": {
    "inputs": {
      "images": [
        "20",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "First Pass"
    }
  },
  "22": {
    "inputs": {
      "upscale_method": "nearest-exact",
      "width": 1024,
      "height": 1024,
      "crop": "disabled",
      "image": [
        "16",
        0
      ]
    },
    "class_type": "ImageScale",
    "_meta": {
      "title": "Upscale Image"
    }
  },
  "26": {
    "inputs": {
      "ckpt_name": "SD_1_5/photon_v1.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "27": {
    "inputs": {
      "text": "person with afro hair, stand in a national park",
      "clip": [
        "26",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "28": {
    "inputs": {
      "text": "ng_deepnegative_v1_75t, holding, worst quality, low quality, normal quality ,lowres, bad anatomy, bad hands,flare,glow, blur, defocus, depth of field ,black and white photo, bad eyes,weird hue colors, CGI, Unreal, Airbrushed, Digital,bad teeth, Blur, Noise, Distortion, Artifacts, Low resolution, Low quality, Overexposure, Underexposure, Unrealistic colors, Cartoonish, Pixelated, Unfocused, Out of frame, Grainy, Inconsistent lighting, Overly saturated, Abstract, Motion blur, Overexaggerated features, CGI, 2D rendering, Sketch, Comic style, Incorrect proportions, Unrealistic textures, Glitch, Blemishes, Overly sharp edges, Chromatic aberration, Unnatural poses, cartoon, comic, painting, drawing, art, computer generated, glow, bald, naked, no cloth, blur background\n",
      "clip": [
        "26",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "29": {
    "inputs": {
      "samples": [
        "30",
        0
      ],
      "vae": [
        "26",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "30": {
    "inputs": {
      "seed": 1036455721550110,
      "steps": 25,
      "cfg": 2,
      "sampler_name": "dpmpp_2m_sde",
      "scheduler": "karras",
      "denoise": 0.9500000000000001,
      "model": [
        "33",
        0
      ],
      "positive": [
        "35",
        0
      ],
      "negative": [
        "35",
        1
      ],
      "latent_image": [
        "31",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "31": {
    "inputs": {
      "pixels": [
        "92",
        0
      ],
      "vae": [
        "26",
        2
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "32": {
    "inputs": {
      "images": [
        "29",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "33": {
    "inputs": {
      "model_path": "IC-Light/iclight_sd15_fc.safetensors",
      "model": [
        "48",
        0
      ]
    },
    "class_type": "LoadAndApplyICLightUnet",
    "_meta": {
      "title": "Load And Apply IC-Light"
    }
  },
  "35": {
    "inputs": {
      "multiplier": 0.185,
      "positive": [
        "27",
        0
      ],
      "negative": [
        "28",
        0
      ],
      "vae": [
        "26",
        2
      ],
      "foreground": [
        "36",
        0
      ]
    },
    "class_type": "ICLightConditioning",
    "_meta": {
      "title": "IC-Light Conditioning"
    }
  },
  "36": {
    "inputs": {
      "pixels": [
        "41",
        0
      ],
      "vae": [
        "26",
        2
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "41": {
    "inputs": {
      "mode": "resize",
      "supersample": "true",
      "resampling": "nearest",
      "rescale_factor": 1,
      "resize_width": 768,
      "resize_height": 768,
      "image": [
        "60",
        0
      ]
    },
    "class_type": "Image Resize",
    "_meta": {
      "title": "Image Resize"
    }
  },
  "42": {
    "inputs": {
      "images": [
        "92",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "43": {
    "inputs": {
      "width": 512,
      "height": 512,
      "interpolation": "lanczos",
      "method": "fill / crop",
      "condition": "always",
      "multiple_of": 8,
      "image": [
        "20",
        0
      ]
    },
    "class_type": "ImageResize+",
    "_meta": {
      "title": "ðŸ”§ Image Resize"
    }
  },
  "44": {
    "inputs": {
      "shape": "circle",
      "frames": 1,
      "location_x": 296,
      "location_y": 330,
      "grow": 0,
      "frame_width": [
        "43",
        1
      ],
      "frame_height": [
        "43",
        2
      ],
      "shape_width": 250,
      "shape_height": 350
    },
    "class_type": "CreateShapeMask",
    "_meta": {
      "title": "Create Shape Mask"
    }
  },
  "45": {
    "inputs": {
      "mask": [
        "47",
        0
      ]
    },
    "class_type": "MaskToImage",
    "_meta": {
      "title": "Convert Mask to Image"
    }
  },
  "46": {
    "inputs": {
      "expand": 0,
      "incremental_expandrate": 0,
      "tapered_corners": true,
      "flip_input": false,
      "blur_radius": 100,
      "lerp_alpha": 1,
      "decay_factor": 1,
      "fill_holes": false,
      "mask": [
        "61",
        0
      ]
    },
    "class_type": "GrowMaskWithBlur",
    "_meta": {
      "title": "Grow Mask With Blur"
    }
  },
  "47": {
    "inputs": {
      "min": 0,
      "max": 1,
      "mask": [
        "46",
        1
      ]
    },
    "class_type": "RemapMaskRange",
    "_meta": {
      "title": "Remap Mask Range"
    }
  },
  "48": {
    "inputs": {
      "weight": 1,
      "weight_type": "linear",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 1,
      "embeds_scaling": "V only",
      "model": [
        "26",
        0
      ],
      "ipadapter": [
        "50",
        0
      ],
      "image": [
        "20",
        0
      ],
      "attn_mask": [
        "124",
        0
      ],
      "clip_vision": [
        "49",
        0
      ]
    },
    "class_type": "IPAdapterAdvanced",
    "_meta": {
      "title": "IPAdapter Advanced"
    }
  },
  "49": {
    "inputs": {
      "clip_name": "CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "Load CLIP Vision"
    }
  },
  "50": {
    "inputs": {
      "ipadapter_file": "ip-adapter_sd15_plus.pth"
    },
    "class_type": "IPAdapterModelLoader",
    "_meta": {
      "title": "IPAdapter Model Loader"
    }
  },
  "51": {
    "inputs": {
      "mode": "resize",
      "supersample": "true",
      "resampling": "lanczos",
      "rescale_factor": 2,
      "resize_width": 768,
      "resize_height": 768,
      "image": [
        "45",
        0
      ]
    },
    "class_type": "Image Resize",
    "_meta": {
      "title": "Image Resize"
    }
  },
  "53": {
    "inputs": {
      "seed": 775528241322275,
      "steps": 20,
      "cfg": 6,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 1,
      "model": [
        "26",
        0
      ],
      "positive": [
        "55",
        0
      ],
      "negative": [
        "28",
        0
      ],
      "latent_image": [
        "54",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "54": {
    "inputs": {
      "pixels": [
        "60",
        0
      ],
      "vae": [
        "26",
        2
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "55": {
    "inputs": {
      "strength": 0.5,
      "conditioning": [
        "27",
        0
      ],
      "control_net": [
        "56",
        0
      ],
      "image": [
        "57",
        0
      ]
    },
    "class_type": "ControlNetApply",
    "_meta": {
      "title": "Apply ControlNet"
    }
  },
  "56": {
    "inputs": {
      "control_net_name": "control_v11p_sd15_canny.pth"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "57": {
    "inputs": {
      "low_threshold": 100,
      "high_threshold": 200,
      "resolution": 512,
      "image": [
        "60",
        0
      ]
    },
    "class_type": "CannyEdgePreprocessor",
    "_meta": {
      "title": "Canny Edge"
    }
  },
  "58": {
    "inputs": {
      "samples": [
        "53",
        0
      ],
      "vae": [
        "26",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "59": {
    "inputs": {
      "images": [
        "20",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "60": {
    "inputs": {
      "mode": "resize",
      "supersample": "true",
      "resampling": "nearest",
      "rescale_factor": 1,
      "resize_width": 768,
      "resize_height": 768,
      "image": [
        "69",
        0
      ]
    },
    "class_type": "Image Resize",
    "_meta": {
      "title": "Image Resize"
    }
  },
  "61": {
    "inputs": {
      "shape": "square",
      "frames": 1,
      "location_x": 256,
      "location_y": 256,
      "grow": 0,
      "frame_width": [
        "43",
        1
      ],
      "frame_height": [
        "43",
        2
      ],
      "shape_width": 256,
      "shape_height": 256
    },
    "class_type": "CreateShapeMask",
    "_meta": {
      "title": "Create Shape Mask"
    }
  },
  "62": {
    "inputs": {
      "image": "/u/gdso/Pictures/syndata_test/depth/depths_0.png",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image Depth"
    }
  },
  "63": {
    "inputs": {
      "upscale_method": "nearest-exact",
      "width": 1024,
      "height": 1024,
      "crop": "disabled",
      "image": [
        "62",
        0
      ]
    },
    "class_type": "ImageScale",
    "_meta": {
      "title": "Upscale Image"
    }
  },
  "65": {
    "inputs": {
      "width": 1024,
      "height": 1024,
      "keep_proportions": true,
      "mask": [
        "165",
        0
      ]
    },
    "class_type": "ResizeMask",
    "_meta": {
      "title": "Resize Mask"
    }
  },
  "66": {
    "inputs": {
      "mask": [
        "65",
        0
      ]
    },
    "class_type": "InvertMask",
    "_meta": {
      "title": "InvertMask"
    }
  },
  "69": {
    "inputs": {
      "image": [
        "20",
        0
      ],
      "mask": [
        "66",
        0
      ]
    },
    "class_type": "InpaintPreprocessor",
    "_meta": {
      "title": "Inpaint Preprocessor"
    }
  },
  "80": {
    "inputs": {
      "images": [
        "69",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "81": {
    "inputs": {
      "width": 1024,
      "height": 1024,
      "keep_proportions": true,
      "mask": [
        "165",
        0
      ]
    },
    "class_type": "ResizeMask",
    "_meta": {
      "title": "Resize Mask"
    }
  },
  "82": {
    "inputs": {
      "mask": [
        "81",
        0
      ]
    },
    "class_type": "InvertMask",
    "_meta": {
      "title": "InvertMask"
    }
  },
  "83": {
    "inputs": {
      "image": [
        "29",
        0
      ],
      "mask": [
        "82",
        0
      ]
    },
    "class_type": "InpaintPreprocessor",
    "_meta": {
      "title": "Inpaint Preprocessor"
    }
  },
  "84": {
    "inputs": {
      "images": [
        "83",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "85": {
    "inputs": {
      "grow_mask_by": 6,
      "pixels": [
        "60",
        0
      ],
      "vae": [
        "26",
        2
      ],
      "mask": [
        "66",
        0
      ]
    },
    "class_type": "VAEEncodeForInpaint",
    "_meta": {
      "title": "VAE Encode (for Inpainting)"
    }
  },
  "87": {
    "inputs": {
      "seed": 882275514340830,
      "steps": 35,
      "cfg": 3.33,
      "sampler_name": "dpmpp_2m",
      "scheduler": "normal",
      "denoise": 0.33,
      "model": [
        "26",
        0
      ],
      "positive": [
        "27",
        0
      ],
      "negative": [
        "28",
        0
      ],
      "latent_image": [
        "85",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "89": {
    "inputs": {
      "mask": [
        "91",
        0
      ]
    },
    "class_type": "MaskToImage",
    "_meta": {
      "title": "Convert Mask to Image"
    }
  },
  "90": {
    "inputs": {
      "expand": -741,
      "incremental_expandrate": 0,
      "tapered_corners": false,
      "flip_input": false,
      "blur_radius": 75,
      "lerp_alpha": 1,
      "decay_factor": 0.2,
      "fill_holes": false,
      "mask": [
        "165",
        0
      ]
    },
    "class_type": "GrowMaskWithBlur",
    "_meta": {
      "title": "Grow Mask With Blur"
    }
  },
  "91": {
    "inputs": {
      "min": 0,
      "max": 1,
      "mask": [
        "90",
        1
      ]
    },
    "class_type": "RemapMaskRange",
    "_meta": {
      "title": "Remap Mask Range"
    }
  },
  "92": {
    "inputs": {
      "mode": "resize",
      "supersample": "true",
      "resampling": "nearest",
      "rescale_factor": 1,
      "resize_width": 768,
      "resize_height": 768,
      "image": [
        "89",
        0
      ]
    },
    "class_type": "Image Resize",
    "_meta": {
      "title": "Image Resize"
    }
  },
  "124": {
    "inputs": {
      "mask": [
        "91",
        0
      ]
    },
    "class_type": "InvertMask",
    "_meta": {
      "title": "InvertMask"
    }
  },
  "128": {
    "inputs": {
      "seed": 110847587192091,
      "steps": 15,
      "cfg": 5,
      "sampler_name": "dpmpp_sde",
      "scheduler": "karras",
      "denoise": 0.5,
      "model": [
        "2",
        0
      ],
      "positive": [
        "12",
        0
      ],
      "negative": [
        "12",
        1
      ],
      "latent_image": [
        "129",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "129": {
    "inputs": {
      "upscale_method": "nearest-exact",
      "scale_by": 1.5,
      "samples": [
        "140",
        0
      ]
    },
    "class_type": "LatentUpscaleBy",
    "_meta": {
      "title": "Upscale Latent By"
    }
  },
  "130": {
    "inputs": {
      "images": [
        "131",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Second Pass(more details)"
    }
  },
  "131": {
    "inputs": {
      "samples": [
        "128",
        0
      ],
      "vae": [
        "2",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "132": {
    "inputs": {
      "conditioning_to_strength": 0.3,
      "conditioning_to": [
        "133",
        0
      ],
      "conditioning_from": [
        "12",
        0
      ]
    },
    "class_type": "ConditioningAverage",
    "_meta": {
      "title": "ConditioningAverage"
    }
  },
  "133": {
    "inputs": {
      "text": "high detailed face,",
      "clip": [
        "2",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "135": {
    "inputs": {
      "model_name": "bbox/face_yolov8m.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "UltralyticsDetectorProvider"
    }
  },
  "136": {
    "inputs": {
      "model_name": "sam_vit_b_01ec64.pth",
      "device_mode": "AUTO"
    },
    "class_type": "SAMLoader",
    "_meta": {
      "title": "SAMLoader (Impact)"
    }
  },
  "137": {
    "inputs": {
      "images": [
        "138",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "more_detailed_final_result"
    }
  },
  "138": {
    "inputs": {
      "guide_size": 1500,
      "guide_size_for": true,
      "max_size": 1500,
      "seed": 479879052251511,
      "steps": 20,
      "cfg": 5,
      "sampler_name": "dpmpp_sde",
      "scheduler": "karras",
      "denoise": 0.35000000000000003,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": true,
      "bbox_threshold": 0.5,
      "bbox_dilation": 10,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 0,
      "sam_threshold": 0.93,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7000000000000001,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "wildcard": "",
      "cycle": 1,
      "inpaint_model": false,
      "noise_mask_feather": 20,
      "image": [
        "131",
        0
      ],
      "model": [
        "2",
        0
      ],
      "clip": [
        "2",
        1
      ],
      "vae": [
        "2",
        2
      ],
      "positive": [
        "132",
        0
      ],
      "negative": [
        "4",
        0
      ],
      "bbox_detector": [
        "135",
        0
      ],
      "sam_model_opt": [
        "136",
        0
      ]
    },
    "class_type": "FaceDetailer",
    "_meta": {
      "title": "FaceDetailer"
    }
  },
  "139": {
    "inputs": {
      "filename_prefix": "synth_ICT_result",
      "images": [
        "138",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "140": {
    "inputs": {
      "pixels": [
        "29",
        0
      ],
      "vae": [
        "2",
        2
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "150": {
    "inputs": {
      "images": [
        "131",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "162": {
    "inputs": {
      "text": "A 60 year old Russian man with long brown hair, eyes,low-key lighting, photorealistic.",
      "clip": [
        "2",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "164": {
    "inputs": {
      "image": "/u/gdso/Pictures/syndata_test/masks/masks_0.png",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image Mask"
    }
  },
  "165": {
    "inputs": {
      "channel": "green",
      "image": [
        "164",
        0
      ]
    },
    "class_type": "ImageToMask",
    "_meta": {
      "title": "Convert Image to Mask"
    }
  }
}
